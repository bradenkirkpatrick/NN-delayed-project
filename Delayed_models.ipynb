{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe7 in position 20: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# get data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcombined_air_travel_data.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m train_set, test_set \u001b[38;5;241m=\u001b[39m train_test_split(data, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/classes/neuralnetworks/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/classes/neuralnetworks/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Documents/classes/neuralnetworks/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/classes/neuralnetworks/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/classes/neuralnetworks/.venv/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32mparsers.pyx:574\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:663\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe7 in position 20: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "data = pd.read_csv(\"combined_air_travel_data.parquet\")\n",
    "train_set, test_set = train_test_split(data, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, precision_recall_curve\n",
    "def precision_at_recall(y, y_pred, *, recall, **kwargs):\n",
    "    \"\"\"\n",
    "    Calculate the precision at a given recall level.\n",
    "\n",
    "    To use this with cross_val_score, you need to use the make_scorer function to\n",
    "    create a scoring function that can be used with cross_val_score. For example:\n",
    "\n",
    "        scorer = make_scorer(precision_at_recall, recall=0.9)  # 0.9 is the minimum recall level\n",
    "        cross_val_score(model, X, y, cv=5, scoring=scorer)\n",
    "\n",
    "    The default will only work for binary classification problems. You must change the\n",
    "    average parameter if you want to use for multiclass classification. For example:\n",
    "\n",
    "        scorer = make_scorer(precision_at_recall, recall=0.9, average=\"micro\")\n",
    "    \"\"\"\n",
    "    return precision_score(y, y_pred, **kwargs) if recall_score(y, y_pred, **kwargs) > recall else 0.0\n",
    "\n",
    "def recall_at_precision(y, y_pred, *, precision, **kwargs):\n",
    "    \"\"\"\n",
    "    Calculate the recall at a given precision level.\n",
    "\n",
    "    To use this with cross_val_score, you need to use the make_scorer function to\n",
    "    create a scoring function that can be used with cross_val_score. For example:\n",
    "\n",
    "        scorer = make_scorer(recall_at_precision, precision=0.9)\n",
    "        cross_val_score(model, X, y, cv=5, scoring=scorer)\n",
    "\n",
    "    The default will only work for binary classification problems. You must change the\n",
    "    average parameter if you want to use for multiclass classification. For example:\n",
    "\n",
    "        scorer = make_scorer(recall_at_precision, precision=0.9, average=\"micro\")\n",
    "    \"\"\"\n",
    "    return recall_score(y, y_pred, **kwargs) if precision_score(y, y_pred, **kwargs) > precision else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature lists\n",
    "weather_features = ['date', 'temperature_2m', 'relative_humidity_2m', 'dew_point_2m',\n",
    "       'precipitation_probability', 'apparent_temperature', 'precipitation',\n",
    "       'rain', 'showers', 'snowfall', 'snow_depth', 'soil_temperature_0cm',\n",
    "       'soil_moisture_0_to_1cm', 'vapour_pressure_deficit',\n",
    "       'et0_fao_evapotranspiration', 'evapotranspiration', 'visibility',\n",
    "       'cloud_cover_mid', 'cloud_cover_high', 'cloud_cover_low', 'cloud_cover',\n",
    "       'surface_pressure', 'weather_code', 'pressure_msl', 'wind_speed_10m',\n",
    "       'wind_speed_80m', 'wind_speed_120m', 'wind_speed_180m',\n",
    "       'wind_direction_10m', 'wind_direction_80m', 'wind_direction_120m',\n",
    "       'wind_direction_180m', 'wind_gusts_10m', 'temperature_80m',\n",
    "       'temperature_120m', 'temperature_180m', 'latitude', 'longitude',\n",
    "       'IATA']\n",
    "\n",
    "airplane_features = [\"id\", \"reg\", \"active\", \"serial\", \"hexIaco\", \"airlineName\", \"icaoCodeShort\", \"iataCode\", \"model\", \"modelCode\", \"numSeats\", \n",
    "\"rolloutDate\", \"firstFlightDate\", \"deliveryDate\", \"registrationDate\", \"typeName\", \"numEngines\", \"engineType\", \"isFreighter\", \n",
    "\"productionLine\", \"ageYears\", \"verified\", \"numRegistrations\", \"firstRegistrationDate\"]\n",
    "\n",
    "\n",
    "flight_features = ['Year', 'Quarter', 'Month', 'DayofMonth', 'DayOfWeek', 'FlightDate',\n",
    "       'Marketing_Airline_Network', 'Operated_or_Branded_Code_Share_Partners',\n",
    "       'DOT_ID_Marketing_Airline', 'IATA_Code_Marketing_Airline',\n",
    "       'Flight_Number_Marketing_Airline',\n",
    "       'Originally_Scheduled_Code_Share_Airline',\n",
    "       'DOT_ID_Originally_Scheduled_Code_Share_Airline',\n",
    "       'IATA_Code_Originally_Scheduled_Code_Share_Airline',\n",
    "       'Flight_Num_Originally_Scheduled_Code_Share_Airline',\n",
    "       'Operating_Airline ', 'DOT_ID_Operating_Airline',\n",
    "       'IATA_Code_Operating_Airline', 'Tail_Number',\n",
    "       'Flight_Number_Operating_Airline', 'OriginAirportID',\n",
    "       'OriginAirportSeqID', 'OriginCityMarketID', 'Origin', 'OriginCityName',\n",
    "       'OriginState', 'OriginStateFips', 'OriginStateName', 'OriginWac',\n",
    "       'DestAirportID', 'DestAirportSeqID', 'DestCityMarketID', 'Dest',\n",
    "       'DestCityName', 'DestState', 'DestStateFips', 'DestStateName',\n",
    "       'DestWac', 'CRSDepTime', 'DepTime', 'DepDelay', 'DepDelayMinutes',\n",
    "       'DepDel15', 'DepartureDelayGroups', 'DepTimeBlk', 'TaxiOut',\n",
    "       'WheelsOff', 'WheelsOn', 'TaxiIn', 'CRSArrTime', 'ArrTime', 'ArrDelay',\n",
    "       'ArrDelayMinutes', 'ArrDel15', 'ArrivalDelayGroups', 'ArrTimeBlk',\n",
    "       'Cancelled', 'CancellationCode', 'Diverted', 'CRSElapsedTime', 'ActualElapsedTime', \n",
    "       'AirTime', 'Flights', 'Distance', 'DistanceGroup',\n",
    "       'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay',\n",
    "       'LateAircraftDelay', 'FirstDepTime', 'TotalAddGTime', 'LongestAddGTime',\n",
    "       'DivAirportLandings', 'DivReachedDest', 'DivActualElapsedTime',\n",
    "       'DivArrDelay', 'DivDistance', 'Div1Airport', 'Div1AirportID',\n",
    "       'Div1AirportSeqID', 'Div1WheelsOn', 'Div1TotalGTime',\n",
    "       'Div1LongestGTime', 'Div1WheelsOff', 'Div1TailNum', 'Div2Airport',\n",
    "       'Div2AirportID', 'Div2AirportSeqID', 'Div2WheelsOn', 'Div2TotalGTime',\n",
    "       'Div2LongestGTime', 'Div2WheelsOff', 'Div2TailNum', 'Div3Airport',\n",
    "       'Div3AirportID', 'Div3AirportSeqID', 'Div3WheelsOn', 'Div3TotalGTime',\n",
    "       'Div3LongestGTime', 'Div3WheelsOff', 'Div3TailNum', 'Div4Airport',\n",
    "       'Div4AirportID', 'Div4AirportSeqID', 'Div4WheelsOn', 'Div4TotalGTime',\n",
    "       'Div4LongestGTime', 'Div4WheelsOff', 'Div4TailNum', 'Div5Airport',\n",
    "       'Div5AirportID', 'Div5AirportSeqID', 'Div5WheelsOn', 'Div5TotalGTime',\n",
    "       'Div5LongestGTime', 'Div5WheelsOff', 'Div5TailNum', 'Duplicate',\n",
    "       'Unnamed: 119']\n",
    "\n",
    "diverted_features = ['DivAirportLandings','DivReachedDest','DivActualElapsedTime','DivArrDelay','DivDistance','Div1Airport', 'Div1AirportID', 'Div1AirportSeqID', 'Div1WheelsOn', 'Div1TotalGTime', 'Div1LongestGTime', 'Div1WheelsOff', 'Div1TailNum', 'Div2Airport', 'Div2AirportID', 'Div2AirportSeqID', 'Div2WheelsOn', 'Div2TotalGTime', 'Div2LongestGTime', 'Div2WheelsOff', 'Div2TailNum', 'Div3Airport', 'Div3AirportID', 'Div3AirportSeqID', 'Div3WheelsOn', 'Div3TotalGTime', 'Div3LongestGTime', 'Div3WheelsOff', 'Div3TailNum', 'Div4Airport', 'Div4AirportID', 'Div4AirportSeqID', 'Div4WheelsOn', 'Div4TotalGTime', 'Div4LongestGTime', 'Div4WheelsOff', 'Div4TailNum', 'Div5Airport', 'Div5AirportID', 'Div5AirportSeqID', 'Div5WheelsOn', 'Div5TotalGTime', 'Div5LongestGTime', 'Div5WheelsOff', 'Div5TailNum']\n",
    "# features that cannot be used because we have to predict 2 weeks ahead of time\n",
    "too_early_features = ['CRSDepTime','DepTime','DepDelay', 'DepDelayMinutes', 'DepDel15', 'DepartureDelayGroups', 'DepTimeBlk', 'TaxiOut', 'WheelsOff', 'WheelsOn', 'TaxiIn', 'ArrTime', 'ArrDelay', 'ArrDelayMinutes', 'ArrDel15', 'ArrivalDelayGroups', 'ArrTimeBlk', 'Cancelled', 'CancellationCode', 'Diverted', 'CRSElapsedTime', 'ActualElapsedTime', 'AirTime', 'Flights', 'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay', 'FirstDepTime', 'TotalAddGTime', 'LongestAddGTime', 'Duplicate', 'Unnamed: 119', 'Originally_Scheduled_Code_Share_Airline','DOT_ID_Originally_Scheduled_Code_Share_Airline', 'IATA_Code_Originally_Scheduled_Code_Share_Airline', 'Flight_Num_Originally_Scheduled_Code_Share_Airline']\n",
    "\n",
    "highly_correlated_weather_features = ['rain', 'temperature_2m', 'precipitation_probability', 'dew_point_2m', 'soil_temperature_0cm', 'soil_moisture_0_to_1cm', 'IATA', 'cloud_cover_low', 'cloud_cover_mid', 'cloud_cover_high', 'wind_speed_80m', 'wind_speed_120m', 'wind_speed_180m', 'temperature_80m', 'temperature_120m', 'temperature_180m', 'wind_direction_80m', 'wind_direction_120m', 'wind_direction_180m', 'wind_speed_10m']\n",
    "\n",
    "highly_correlated_with_other_features  = ['DistanceGroup'] + highly_correlated_weather_features\n",
    "\n",
    "categorical_features = ['IATA']\n",
    "numerical_features = weather_features\n",
    "drop_features = diverted_features + too_early_features + highly_correlated_with_other_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "main_pipeline = Pipeline([\n",
    "    ('preprocessor', ColumnTransformer([\n",
    "        ('cat', categorical_pipeline, categorical_features),\n",
    "        ('num', numerical_pipeline, numerical_pipeline)\n",
    "        ('drop', 'drop', drop_features)\n",
    "    ])),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- drop un-useable attribute\n",
    "- split weather code\n",
    "- percip and rain corelate 100% with each other keep percip\n",
    "- distance and distance group corelate 99% with each other keep distance\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Attribute             | Type        | Missing | Noise | Use  | Distribution             |\n",
      "| --------------------- | ----------- | ------- | ----- | ---- | ------------------------ |\n",
      "|date|\n",
      "|temperature_2m|\n",
      "|relative_humidity_2m|\n",
      "|dew_point_2m|\n",
      "|precipitation_probability|\n",
      "|apparent_temperature|\n",
      "|precipitation|\n",
      "|rain|\n",
      "|showers|\n",
      "|snowfall|\n",
      "|snow_depth|\n",
      "|soil_temperature_0cm|\n",
      "|soil_moisture_0_to_1cm|\n",
      "|vapour_pressure_deficit|\n",
      "|et0_fao_evapotranspiration|\n",
      "|evapotranspiration|\n",
      "|visibility|\n",
      "|cloud_cover_mid|\n",
      "|cloud_cover_high|\n",
      "|cloud_cover_low|\n",
      "|cloud_cover|\n",
      "|surface_pressure|\n",
      "|weather_code|\n",
      "|pressure_msl|\n",
      "|wind_speed_10m|\n",
      "|wind_speed_80m|\n",
      "|wind_speed_120m|\n",
      "|wind_speed_180m|\n",
      "|wind_direction_10m|\n",
      "|wind_direction_80m|\n",
      "|wind_direction_120m|\n",
      "|wind_direction_180m|\n",
      "|wind_gusts_10m|\n",
      "|temperature_80m|\n",
      "|temperature_120m|\n",
      "|temperature_180m|\n",
      "|latitude|\n",
      "|longitude|\n",
      "|IATA|\n"
     ]
    }
   ],
   "source": [
    "print(\"| Attribute             | Type        | Missing | Noise | Use  | Distribution             |\")\n",
    "print(\"| --------------------- | ----------- | ------- | ----- | ---- | ------------------------ |\")\n",
    "for feature in weather_features:\n",
    "    print(f\"|{feature}|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifiers to Try for Project\n",
    "==============================\n",
    "List of classifiers to try along with their most critical hyperparameters. You should expect your short listing of models to take on the order of a day and your hyperparameter tuning multiple days.\n",
    "\n",
    "Note: you may want to look into each one briefly just to get some info like if they are appropriate for our dataset (considering the number of samples vs features). You will want to read the documentation for default values and understanding what ranges may be useful to examine.\n",
    "\n",
    " \n",
    "\n",
    "Linear Classifiers\n",
    "------------------\n",
    "\n",
    "All of these have a coef_ parameter which can be useful during exploration, especially if the penalty is 'l1' which makes the coefficient of unimportant features (near) 0 (but the models all require properly scaled data for the coefficients to be meaningful).\n",
    "\n",
    "LogisticRegression\n",
    "penalty: l2 (default), l1 (for sparse models), elasticnet, or none\n",
    "C: 1 (default), decrease for regularization\n",
    "dual: False (default)\n",
    "solver: different solvers are better for different problems\n",
    "SGDClassifier [basically an online version of LogisticRegression, if online is not needed then you probably don't need this]\n",
    "loss: hinge (default, like LinearSVC) or log (like LogisticRegression)\n",
    "penalty: l2 (default), l1, elasticnet, or none\n",
    "alpha: default 0.0001, increase for regularization\n",
    "l1_ratio: ratio of Lasso vs Ridge if penalty='elasticnet'\n",
    "max_iter/tol: max number of steps to attempt and target tolerance to achieve\n",
    "learning_rate: constant, optimal, invscaling (default), adaptive\n",
    "eta0: initial learning rate, default is 0.01\n",
    "shuffle: default True, shuffle data between iterations\n",
    "early_stopping/validation_fraction/n_iter_no_change: early stopping regularization\n",
    " \n",
    "\n",
    "Neural Network Classifiers\n",
    "--------------------------\n",
    "\n",
    "MLPClassifier (i.e. neural network)\n",
    "hidden_layer_sizes: default is (100,)\n",
    "activation: 'relu' (default), 'identity', 'logistic', 'tanh'\n",
    "alpha: default 0.0001, increase for regularization (always L2)\n",
    "max_iter/tol: max number of steps to attempt and target tolerance to achieve\n",
    "learning_rate_init: initial learning rate, default is 0.001\n",
    "batch_size: sizes of batches\n",
    "shuffle: default True, shuffle data between iterations\n",
    "early_stopping/validation_fraction/n_iter_no_change: early stopping regularization\n",
    " \n",
    "\n",
    "Tree-Based Classifiers\n",
    "----------------------\n",
    "\n",
    "All of these have a feature_importances_ parameter which can be useful during exploration. Scaling does not need to be done for that parameter to have meaning.\n",
    "\n",
    "DecisionTreeClassifier\n",
    "criterion: gini (default) or entropy\n",
    "splitter: best (default) or random (faster)\n",
    "max_depth, , max_leaf_nodes, min_samples_split, min_samples_leaf, min_impurity_split, etc: control tree generation, decrease max_* to regularize, increase min_* to regularize\n",
    "presort: setting to True can increase speed for small datasets or restricted depths\n",
    "RandomForestClassifier and ExtraTreesClassifier\n",
    "n_estimators: default 100\n",
    "Supports all hyperparameters of DecisionTreeClassifier listed above except splitter (always best) and presort (always False)\n",
    "max_features defaults to sqrt, also supports log2, int (for a count), or float (for a percentage)\n",
    "max_samples & bootstrap: default all samples with bootstrapping\n",
    "GradientBoostingClassifier\n",
    "or XGBClassifierLinks to an external site. - improved version but requires an external library to be installed and has a bit difference hyperparameters\n",
    "learning_rate: default 0.1, lower to increase regularization, higher to go faster\n",
    "n_estimators: default 100, balance with learning rate, can be fairly high though\n",
    "subsample: default is 1.0, values <1.0 enable stochastic gradient boosting\n",
    "n_iter_no_change/validation_fraction: early stopping regularization\n",
    "Supports most max_* and min_* hyperparameters of DecisionTreeClassifier listed above\n",
    "max_features defaults to sqrt, also supports log2, int (for a count), or float (for a percentage)\n",
    " \n",
    "\n",
    "Instance-Based Classifiers\n",
    "--------------------------\n",
    "\n",
    "KNeighborsClassifier\n",
    "n_neighbors: 5 (default)\n",
    "weights: 'uniform' (default) or 'distance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
